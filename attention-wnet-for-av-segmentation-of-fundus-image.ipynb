{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Install Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-09-22T09:19:50.891919Z",
     "iopub.status.busy": "2024-09-22T09:19:50.890977Z",
     "iopub.status.idle": "2024-09-22T09:20:12.920690Z",
     "shell.execute_reply": "2024-09-22T09:20:12.919406Z",
     "shell.execute_reply.started": "2024-09-22T09:19:50.891883Z"
    },
    "id": "SxrFOZtZhXVU",
    "outputId": "78de485e-544a-459a-9782-022ccccddde1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-unet-collection in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.1.13)\n",
      "Requirement already satisfied: focal_loss in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: tensorflow>=2.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from focal_loss) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow>=2.2->focal_loss) (2.12.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (21.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (1.54.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (23.3.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (16.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (2.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (2.12.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (1.6.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (0.4.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (0.4.8)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (0.31.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (1.14.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (65.5.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (1.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (4.12.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (3.20.3)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (2.12.2)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (1.23.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (0.37.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (0.1.0)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (1.9.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (2.17.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (0.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (1.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (3.0.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (2.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow>=2.2->focal_loss) (3.2.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-addons) (21.3)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from packaging->tensorflow-addons) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "#@title Install Modules\n",
    "!pip install keras-unet-collection focal_loss \n",
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Path Definition and DataGenerator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-09-22T09:20:12.924186Z",
     "iopub.status.busy": "2024-09-22T09:20:12.923374Z",
     "iopub.status.idle": "2024-09-22T09:20:12.934185Z",
     "shell.execute_reply": "2024-09-22T09:20:12.933354Z",
     "shell.execute_reply.started": "2024-09-22T09:20:12.924145Z"
    },
    "id": "ez3jAAExiEep",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "a0146e6d-6f61-4dd8-be16-ff3e8e456371"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#@title Import Modules\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "#Necessary Packages\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "import gc\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras_unet_collection import models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pickle\n",
    "from focal_loss import BinaryFocalLoss\n",
    "from IPython.display import clear_output\n",
    "from keras_unet_collection.layer_utils import *\n",
    "from keras_unet_collection.transformer_layers import patch_extract, patch_embedding, SwinTransformerBlock, patch_merging, patch_expanding\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-09-22T09:20:12.936142Z",
     "iopub.status.busy": "2024-09-22T09:20:12.935726Z",
     "iopub.status.idle": "2024-09-22T09:20:12.970320Z",
     "shell.execute_reply": "2024-09-22T09:20:12.969243Z",
     "shell.execute_reply.started": "2024-09-22T09:20:12.936108Z"
    },
    "id": "OKBFFaD2iNE-",
    "outputId": "505df328-8423-412e-8e75-fb01f778383d"
   },
   "outputs": [],
   "source": [
    "# @title Path Definition and DataGenerator\n",
    "Image_Size = 512\n",
    "Dataset = \"DRIVE_AV\" \n",
    "Train_Image_Path = \"/kaggle/input/augmenteddataset/\"+Dataset+\"/images/\"\n",
    "Train_Label_Path = \"/kaggle/input/augmenteddataset/\"+Dataset+\"/label/\"\n",
    "\n",
    "Test_Image_Path = \"/kaggle/input/drive-av/test/images/\"\n",
    "Test_Label_Path = \"/kaggle/input/drive-av/test/label/\"\n",
    "\n",
    "# Train_Image_Path = \"/home/subin/avSegmentation/data/HRF/train/images/\"\n",
    "# Train_Label_Path = \"/home/subin/avSegmentation/data/HRF/train/label/\"\n",
    "\n",
    "# Test_Image_Path = \"/home/subin/avSegmentation/data/HRF/test/images/\"\n",
    "# Test_Label_Path = \"/home/subin/avSegmentation/data/HRF/test/label/\"\n",
    "\n",
    "class MyDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, data_path, label_path, channel_type, batch_size=32, shuffle=True, augment_data=False):\n",
    "        self.data_path = data_path\n",
    "        self.label_path = label_path\n",
    "        self.channel_type = channel_type\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.image_files = os.listdir(data_path)\n",
    "        self.indexes = np.arange(len(self.image_files))\n",
    "        self.on_epoch_end()\n",
    "        self.beforeClahe = None\n",
    "        self.augment_data = augment_data\n",
    "        \n",
    "        self.datagen = ImageDataGenerator(\n",
    "            rotation_range=20,      # randomly rotate images by up to 20 degrees\n",
    "            width_shift_range=0.2,  # randomly shift images horizontally by up to 20% of width\n",
    "            height_shift_range=0.2, # randomly shift images vertically by up to 20% of height\n",
    "            horizontal_flip=True,   # randomly flip images horizontally\n",
    "            fill_mode='nearest'     # fill in missing pixels with the nearest value\n",
    "#             preprocessing_function=lambda x: x + 0.05 * np.random.randn(*x.shape)\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_files) / self.batch_size))\n",
    "\n",
    "    def __convertlabelto4channel__(self, y):\n",
    "        y_converted = np.zeros((y.shape[0], y.shape[1], y.shape[2], 4))\n",
    "        for i in range(y.shape[0]):\n",
    "            y_converted[i, :, :, 0] = np.all(y[i, :, :] == [0, 0, 0], axis=-1)\n",
    "            y_converted[i, :, :, 1] = np.all(y[i, :, :] == [255, 0, 0], axis=-1)\n",
    "            y_converted[i, :, :, 2] = np.all(y[i, :, :] == [0, 255, 0], axis=-1)\n",
    "            y_converted[i, :, :, 3] = np.all(y[i, :, :] == [0, 0, 255], axis=-1)\n",
    "        return y_converted\n",
    "    \n",
    "    def convert_to_rgb(self, softmax_values):\n",
    "        argmax_indices = np.argmax(softmax_values, axis=-1)\n",
    "        indices = np.unravel_index(argmax_indices, softmax_values.shape[:-1])\n",
    "        y = np.zeros_like(softmax_values)\n",
    "        y[np.arange(y.shape[0])[:, None, None], indices[0], indices[1], argmax_indices] = 1\n",
    "        # Convert categorical labels back to 3-channel RGB format\n",
    "        rgb_labels = np.zeros((y.shape[0], y.shape[1], y.shape[2], 3), dtype=np.uint8)\n",
    "        rgb_labels[np.all(y == [0, 1, 0, 0], axis=-1)] = [255, 0, 0]  # Red label\n",
    "        rgb_labels[np.all(y == [0, 0, 1, 0], axis=-1)] = [0, 255, 0]  # Green label\n",
    "        rgb_labels[np.all(y == [0, 0, 0, 1], axis=-1)] = [0, 0, 255]  # Blue label\n",
    "        rgb_labels[np.all(y == [1, 0, 0, 0], axis=-1)] = [0, 0, 0]  # Background label\n",
    "        return rgb_labels\n",
    "\n",
    "    def getRawIInput(self):\n",
    "        return self.beforeClahe\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X, y = self.__data_generation(indexes)\n",
    "        if(self.augment_data):\n",
    "            X, y = self.apply_transforms(X, y)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        \n",
    "        X = np.empty((len(indexes), Image_Size, Image_Size,1))\n",
    "        y = np.empty((len(indexes), Image_Size, Image_Size, 3))\n",
    "        self.beforeClahe = np.empty((len(indexes), Image_Size, Image_Size))\n",
    "\n",
    "        for i, index in enumerate(indexes):\n",
    "            # Load and pre-process image\n",
    "            img_path = os.path.join(self.data_path, self.image_files[index])\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (Image_Size,Image_Size), interpolation = cv2.INTER_NEAREST)\n",
    "\n",
    "            if(self.channel_type==\"red\"):\n",
    "                img = img[:,:,2]\n",
    "            elif(self.channel_type==\"green\"):\n",
    "                img = img[:,:,1]\n",
    "            elif(self.channel_type==\"blue\"):\n",
    "                img = img[:,:,0]\n",
    "            elif(self.channel_type==\"gray\"):\n",
    "               img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            self.beforeClahe[i,] = img\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "            img = clahe.apply(img)\n",
    "            \n",
    "            img = np.reshape(img, (Image_Size, Image_Size, 1))\n",
    "            mean = np.mean(img)\n",
    "            std = np.std(img)\n",
    "            normalized_img_data = (img - mean) / std\n",
    "            X[i,] = normalized_img_data\n",
    "\n",
    "\n",
    "            # Load label image\n",
    "            \n",
    "            label_path = os.path.join(self.label_path, self.image_files[index])\n",
    "            label_img = cv2.imread(label_path)\n",
    "            label_img = cv2.resize(label_img, (Image_Size,Image_Size), interpolation = cv2.INTER_NEAREST)\n",
    "            y[i,] = label_img\n",
    "        y = self.__convertlabelto4channel__(y)\n",
    "        \n",
    "        return X, y\n",
    "    def apply_transforms(self, batch_x, batch_y):\n",
    "        seed = np.random.randint(0, 100000)\n",
    "        \n",
    "        # apply the same set of transformations to both the input and output data\n",
    "        x_transformed = self.datagen.flow(batch_x, batch_size=self.batch_size, shuffle=False, seed=seed).next()\n",
    "        y_transformed = self.datagen.flow(batch_y, batch_size=self.batch_size, shuffle=False, seed=seed).next()\n",
    "        \n",
    "        return x_transformed, y_transformed\n",
    "    def combine_and_split_data(self, train_images, train_labels, test_images, test_labels, split_percentage):\n",
    "        \"\"\"\n",
    "        Combine train and test data and split them into new train and test data based on split percentage given\n",
    "\n",
    "        Parameters:\n",
    "        train_images (numpy.ndarray): Array of training images\n",
    "        train_labels (numpy.ndarray): Array of training labels\n",
    "        test_images (numpy.ndarray): Array of testing images\n",
    "        test_labels (numpy.ndarray): Array of testing labels\n",
    "        split_percentage (float): Percentage of data to be used for training, between 0 and 1\n",
    "\n",
    "        Returns:\n",
    "        Tuple of four numpy arrays: new_train_images, new_train_labels, new_test_images, new_test_labels\n",
    "        \"\"\"\n",
    "\n",
    "        # Combine the train and test data into a single set\n",
    "        all_images = np.concatenate((train_images, test_images), axis=0)\n",
    "        all_labels = np.concatenate((train_labels, test_labels), axis=0)\n",
    "\n",
    "        # Get the total number of examples in the data\n",
    "        num_examples = all_images.shape[0]\n",
    "\n",
    "        # Calculate the number of examples to use for training and testing\n",
    "        num_train = int(num_examples * split_percentage)\n",
    "        num_test = num_examples - num_train\n",
    "\n",
    "        # Shuffle the data\n",
    "        shuffle_indices = np.random.permutation(num_examples)\n",
    "        all_images = all_images[shuffle_indices]\n",
    "        all_labels = all_labels[shuffle_indices]\n",
    "\n",
    "        # Split the data into train and test sets\n",
    "        new_train_images = all_images[:num_train]\n",
    "        new_train_labels = all_labels[:num_train]\n",
    "        new_test_images = all_images[num_train:]\n",
    "        new_test_labels = all_labels[num_train:]\n",
    "\n",
    "        return new_train_images, new_train_labels, new_test_images, new_test_labels\n",
    "\n",
    "\n",
    "\n",
    "# train_generator = MyDataGenerator(Train_Image_Path, Train_Label_Path, \"red\", batch_size=30, shuffle = True, augment_data = False) \n",
    "# X, y = train_generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T09:20:12.973544Z",
     "iopub.status.busy": "2024-09-22T09:20:12.973184Z",
     "iopub.status.idle": "2024-09-22T09:20:13.138008Z",
     "shell.execute_reply": "2024-09-22T09:20:13.137198Z",
     "shell.execute_reply.started": "2024-09-22T09:20:12.973519Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/augmenteddataset/DRIVEAUG.pickle\", \"rb\") as f:\n",
    "    RawTrainX, RawTrainY = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T09:20:13.139459Z",
     "iopub.status.busy": "2024-09-22T09:20:13.139151Z",
     "iopub.status.idle": "2024-09-22T09:20:13.144108Z",
     "shell.execute_reply": "2024-09-22T09:20:13.143073Z",
     "shell.execute_reply.started": "2024-09-22T09:20:13.139434Z"
    }
   },
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=X.shape[0], ncols=2, figsize=(10, 20))\n",
    "\n",
    "# for i in range(X.shape[0]):\n",
    "#     axes[i, 0].imshow(X[i])\n",
    "#     axes[i, 1].imshow(y[i][:,:,1:4])\n",
    "    \n",
    "#     # set titles as labels\n",
    "#     axes[i, 0].set_title(f\"Image {i+1}\")\n",
    "#     axes[i, 1].set_title(f\"Label {i+1}\")\n",
    "    \n",
    "#     # remove ticks\n",
    "#     axes[i, 0].set_xticks([])\n",
    "#     axes[i, 0].set_yticks([])\n",
    "#     axes[i, 1].set_xticks([])\n",
    "#     axes[i, 1].set_yticks([])\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2024-09-22T09:20:13.145662Z",
     "iopub.status.busy": "2024-09-22T09:20:13.145310Z",
     "iopub.status.idle": "2024-09-22T09:20:13.157131Z",
     "shell.execute_reply": "2024-09-22T09:20:13.156205Z",
     "shell.execute_reply.started": "2024-09-22T09:20:13.145636Z"
    },
    "id": "QJVZjtKcpRsX",
    "outputId": "956b83b0-4886-4734-e4e4-cc6a8e06e4bd"
   },
   "outputs": [],
   "source": [
    "# # @title Display first few images and labels\n",
    "# train_generator = MyDataGenerator(Train_Image_Path, Train_Label_Path, \"red\", batch_size=16, shuffle = True) \n",
    "# X, y = train_generator.__getitem__(0)\n",
    "# y_images = train_generator.convert_to_rgb(y)\n",
    "# Raw_Input_Image = train_generator.getRawIInput()\n",
    "# fig, axs = plt.subplots(nrows=3, ncols=4, figsize=(20, 20))\n",
    "\n",
    "# for i in range(4):\n",
    "#     axs[0, i].imshow(Raw_Input_Image[i])\n",
    "#     axs[0, i].set_title('Raw Image '+ str(i))\n",
    "\n",
    "#     axs[1, i].imshow(X[i], cmap='gray')\n",
    "#     axs[1, i].set_title('After Clahe '+ str(i))\n",
    "\n",
    "#     axs[2, i].imshow(y[i])\n",
    "#     axs[2, i].set_title('Label '+ str(i))\n",
    "\n",
    "\n",
    "# # Adjust the spacing between subplots\n",
    "# fig.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2024-09-22T09:20:13.158994Z",
     "iopub.status.busy": "2024-09-22T09:20:13.158394Z",
     "iopub.status.idle": "2024-09-22T09:20:13.167069Z",
     "shell.execute_reply": "2024-09-22T09:20:13.166294Z",
     "shell.execute_reply.started": "2024-09-22T09:20:13.158945Z"
    },
    "id": "F9vViQVTpSXv",
    "outputId": "e35583c8-e85d-4892-88b7-10b391cb6db0"
   },
   "outputs": [],
   "source": [
    "# # @title Display first few images and labels of all channels\n",
    "\n",
    "# for channelType in ([\"red\", \"green\", \"blue\", \"gray\"]):\n",
    "#     train_generator = MyDataGenerator(Train_Image_Path, Train_Label_Path, channelType, batch_size=16, shuffle = True)\n",
    "\n",
    "#     X, y = train_generator.__getitem__(0)\n",
    "#     y_images = train_generator.convert_to_rgb(y)\n",
    "#     Raw_Input_Image = train_generator.getRawIInput()\n",
    "#     fig, axs = plt.subplots(nrows=3, ncols=4, figsize=(20, 20))\n",
    "\n",
    "#     for i in range(4):\n",
    "#         axs[0, i].imshow(Raw_Input_Image[i])\n",
    "#         axs[0, i].set_title('Raw Image '+ str(i))\n",
    "\n",
    "#         axs[1, i].imshow(X[i])\n",
    "#         axs[1, i].set_title('After Clahe '+ str(i))\n",
    "\n",
    "#         axs[2, i].imshow(y[i])\n",
    "#         axs[2, i].set_title('Label '+ str(i))\n",
    "\n",
    "#     fig.suptitle('Channel Type='+channelType, fontsize=8)\n",
    "#     # Adjust the spacing between subplots\n",
    "#     fig.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "\n",
    "#     # Show the plot\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Attention UNet Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-09-22T09:20:13.168437Z",
     "iopub.status.busy": "2024-09-22T09:20:13.168162Z",
     "iopub.status.idle": "2024-09-22T09:20:13.194328Z",
     "shell.execute_reply": "2024-09-22T09:20:13.193373Z",
     "shell.execute_reply.started": "2024-09-22T09:20:13.168415Z"
    },
    "id": "I0x3yMi1ogJd",
    "outputId": "a5c91cf5-cb2d-4105-9ce7-712724eb4f72"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "from keras_unet_collection.layer_utils import *\n",
    "from keras_unet_collection.activations import GELU, Snake\n",
    "from keras_unet_collection._model_unet_2d import UNET_left, UNET_right\n",
    "from keras_unet_collection._backbone_zoo import backbone_zoo, bach_norm_checker\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def UNET_att_right(X, X_left, channel, att_channel, kernel_size=3, stack_num=2,\n",
    "                   activation='ReLU', atten_activation='ReLU', attention='add',\n",
    "                   unpool=True, batch_norm=False, name='right0'):\n",
    "    \n",
    "    pool_size = 2\n",
    "    \n",
    "    X = decode_layer(X, channel, pool_size, unpool, \n",
    "                     activation=activation, batch_norm=batch_norm, name='{}_decode'.format(name))\n",
    "    \n",
    "    X_left = attention_gate(X=X_left, g=X, channel=att_channel, activation=atten_activation, \n",
    "                            attention=attention, name='{}_att'.format(name))\n",
    "    \n",
    "    # Tensor concatenation\n",
    "    H = concatenate([X, X_left], axis=-1, name='{}_concat'.format(name))\n",
    "    \n",
    "    # stacked linear convolutional layers after concatenation\n",
    "    H = CONV_stack(H, channel, kernel_size, stack_num=stack_num, activation=activation, \n",
    "                   batch_norm=batch_norm, name='{}_conv_after_concat'.format(name))\n",
    "    \n",
    "    return H\n",
    "\n",
    "def att_unet_2d_base(input_tensor, filter_num, stack_num_down=2, stack_num_up=2,\n",
    "                     activation='ReLU', atten_activation='ReLU', attention='add', batch_norm=False, pool=True, unpool=True, \n",
    "                     backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='attunet'):\n",
    "\n",
    "    activation_func = eval(activation)\n",
    "\n",
    "    depth_ = len(filter_num)\n",
    "    X_skip = []\n",
    "\n",
    "    # no backbone cases\n",
    "    if backbone is None:\n",
    "        X = input_tensor\n",
    "        # downsampling blocks\n",
    "        X = CONV_stack(X, filter_num[0], stack_num=stack_num_down, activation=activation, \n",
    "                       batch_norm=batch_norm, name='{}_down0'.format(name))\n",
    "        X_skip.append(X)\n",
    "\n",
    "        for i, f in enumerate(filter_num[1:]):\n",
    "            X = UNET_left(X, f, stack_num=stack_num_down, activation=activation, pool=pool, \n",
    "                          batch_norm=batch_norm, name='{}_down{}'.format(name, i+1))        \n",
    "            X_skip.append(X)\n",
    "\n",
    "    else:\n",
    "        # handling VGG16 and VGG19 separately\n",
    "        if 'VGG' in backbone:\n",
    "            backbone_ = backbone_zoo(backbone, weights, input_tensor, depth_, freeze_backbone, freeze_batch_norm)\n",
    "            # collecting backbone feature maps\n",
    "            X_skip = backbone_([input_tensor,])\n",
    "            depth_encode = len(X_skip)\n",
    "\n",
    "        # for other backbones\n",
    "        else:\n",
    "            backbone_ = backbone_zoo(backbone, weights, input_tensor, depth_-1, freeze_backbone, freeze_batch_norm)\n",
    "            # collecting backbone feature maps\n",
    "            X_skip = backbone_([input_tensor,])\n",
    "            depth_encode = len(X_skip) + 1\n",
    "\n",
    "        # extra conv2d blocks are applied\n",
    "        # if downsampling levels of a backbone < user-specified downsampling levels\n",
    "        if depth_encode < depth_:\n",
    "\n",
    "            # begins at the deepest available tensor  \n",
    "            X = X_skip[-1]\n",
    "\n",
    "            # extra downsamplings\n",
    "            for i in range(depth_-depth_encode):\n",
    "                i_real = i + depth_encode\n",
    "\n",
    "                X = UNET_left(X, filter_num[i_real], stack_num=stack_num_down, activation=activation, pool=pool, \n",
    "                              batch_norm=batch_norm, name='{}_down{}'.format(name, i_real+1))\n",
    "                X_skip.append(X)\n",
    "\n",
    "    # reverse indexing encoded feature maps\n",
    "    X_skip = X_skip[::-1]\n",
    "    # upsampling begins at the deepest available tensor\n",
    "    X = X_skip[0]\n",
    "    # other tensors are preserved for concatenation\n",
    "    X_decode = X_skip[1:]\n",
    "    depth_decode = len(X_decode)\n",
    "\n",
    "    # reverse indexing filter numbers\n",
    "    filter_num_decode = filter_num[:-1][::-1]\n",
    "\n",
    "    for i in range(depth_decode):\n",
    "        f = filter_num_decode[i]\n",
    "\n",
    "        X = UNET_att_right(X, X_decode[i], f, att_channel=f//2, stack_num=stack_num_up,\n",
    "                           activation=activation, atten_activation=atten_activation, attention=attention,\n",
    "                           unpool=unpool, batch_norm=batch_norm, name='{}_up{}'.format(name, i))\n",
    "\n",
    "    # if tensors for concatenation is not enough\n",
    "    # then use upsampling without concatenation \n",
    "    if depth_decode < depth_-1:\n",
    "        for i in range(depth_-depth_decode-1):\n",
    "            i_real = i + depth_decode\n",
    "            X = UNET_right(X, None, filter_num_decode[i_real], stack_num=stack_num_up, activation=activation, \n",
    "                       unpool=unpool, batch_norm=batch_norm, concat=False, name='{}_up{}'.format(name, i_real)) \n",
    "    return X\n",
    "\n",
    "def att_unet_2d(input_size, filter_num, n_labels, stack_num_down=2, stack_num_up=2, activation='ReLU', \n",
    "                atten_activation='ReLU', attention='add', output_activation='Softmax', batch_norm=False, pool=True, unpool=True, \n",
    "                backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='attunet'):\n",
    "   \n",
    "    \n",
    "    # one of the ReLU, LeakyReLU, PReLU, ELU\n",
    "    activation_func = eval(activation)\n",
    "    \n",
    "    if backbone is not None:\n",
    "        bach_norm_checker(backbone, batch_norm)\n",
    "    \n",
    "    IN = Input(input_size)\n",
    "    \n",
    "    # base\n",
    "    X = att_unet_2d_base(IN, filter_num, stack_num_down=stack_num_down, stack_num_up=stack_num_up,\n",
    "                         activation=activation, atten_activation=atten_activation, attention=attention,\n",
    "                         batch_norm=batch_norm, pool=pool, unpool=unpool, \n",
    "                         backbone=backbone, weights=weights, freeze_backbone=freeze_backbone, \n",
    "                         freeze_batch_norm=freeze_backbone, name=name)\n",
    "    \n",
    "    # output layer\n",
    "    OUT = CONV_output(X, n_labels, kernel_size=1, activation=output_activation, name='{}_output'.format(name))\n",
    "    \n",
    "    # functional API model\n",
    "    model = Model(inputs=[IN,], outputs=[OUT,], name='{}_model'.format(name))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Attention WNet Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-09-22T09:20:13.195935Z",
     "iopub.status.busy": "2024-09-22T09:20:13.195570Z",
     "iopub.status.idle": "2024-09-22T09:20:13.206258Z",
     "shell.execute_reply": "2024-09-22T09:20:13.205360Z",
     "shell.execute_reply.started": "2024-09-22T09:20:13.195904Z"
    },
    "id": "eBBK3NSCow26",
    "outputId": "06200e36-61a2-4928-f6bc-969c399ada6e"
   },
   "outputs": [],
   "source": [
    "def att_wnet_2d(input_size, filter_num, n_labels1, n_labels2, stack_num_down=2, stack_num_up=2, activation='ReLU', \n",
    "                atten_activation='ReLU', attention='add', output_activation='Softmax', batch_norm=False, pool=True, unpool=True, \n",
    "                backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='attunet'):\n",
    "    \n",
    "    \n",
    "    # one of the ReLU, LeakyReLU, PReLU, ELU\n",
    "    activation_func = eval(activation)\n",
    "    \n",
    "    if backbone is not None:\n",
    "        bach_norm_checker(backbone, batch_norm)\n",
    "    \n",
    "    IN = Input(input_size)\n",
    "    \n",
    "    # base\n",
    "    X = att_unet_2d_base(IN, filter_num, stack_num_down=stack_num_down, stack_num_up=stack_num_up,\n",
    "                         activation=activation, atten_activation=atten_activation, attention=attention,\n",
    "                         batch_norm=batch_norm, pool=pool, unpool=unpool, \n",
    "                         backbone=backbone, weights=weights, freeze_backbone=freeze_backbone, \n",
    "                         freeze_batch_norm=freeze_backbone, name=name)\n",
    "    \n",
    "    # output layer\n",
    "    OUT = CONV_output(X, n_labels1, kernel_size=1, activation=output_activation, name='{}_output'.format(name))\n",
    "    \n",
    "    concatLayer = tf.keras.layers.Concatenate()([IN, OUT])\n",
    "    \n",
    "    new_X = att_unet_2d_base(concatLayer, filter_num, stack_num_down=stack_num_down, stack_num_up=stack_num_up,\n",
    "                         activation=activation, atten_activation=atten_activation, attention=attention,\n",
    "                         batch_norm=batch_norm, pool=pool, unpool=unpool, \n",
    "                         backbone=backbone, weights=weights, freeze_backbone=freeze_backbone, \n",
    "                         freeze_batch_norm=freeze_backbone, name=name+\"second\")\n",
    "    OUT2 = CONV_output(new_X, n_labels2,kernel_size=1, activation=output_activation, name='{}_output'.format(name+\"second\"))\n",
    "    \n",
    "    # functional API model\n",
    "    model = Model(inputs=[IN,], outputs=[OUT2,], name='{}_model'.format(name))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Swin UNet Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T09:20:13.210955Z",
     "iopub.status.busy": "2024-09-22T09:20:13.210696Z",
     "iopub.status.idle": "2024-09-22T09:20:13.235213Z",
     "shell.execute_reply": "2024-09-22T09:20:13.234324Z",
     "shell.execute_reply.started": "2024-09-22T09:20:13.210932Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def swin_transformer_stack(X, stack_num, embed_dim, num_patch, num_heads, window_size, num_mlp, shift_window=True, name=''):\n",
    "\n",
    "    # Turn-off dropouts\n",
    "    mlp_drop_rate = 0 # Droupout after each MLP layer\n",
    "    attn_drop_rate = 0 # Dropout after Swin-Attention\n",
    "    proj_drop_rate = 0 # Dropout at the end of each Swin-Attention block, i.e., after linear projections\n",
    "    drop_path_rate = 0 # Drop-path within skip-connections\n",
    "    \n",
    "    qkv_bias = True # Convert embedded patches to query, key, and values with a learnable additive value\n",
    "    qk_scale = None # None: Re-scale query based on embed dimensions per attention head # Float for user specified scaling factor\n",
    "    \n",
    "    if shift_window:\n",
    "        shift_size = window_size // 2\n",
    "    else:\n",
    "        shift_size = 0\n",
    "    \n",
    "    for i in range(stack_num):\n",
    "    \n",
    "        if i % 2 == 0:\n",
    "            shift_size_temp = 0\n",
    "        else:\n",
    "            shift_size_temp = shift_size\n",
    "\n",
    "        X = SwinTransformerBlock(dim=embed_dim, num_patch=num_patch, num_heads=num_heads, \n",
    "                                 window_size=window_size, shift_size=shift_size_temp, num_mlp=num_mlp, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 mlp_drop=mlp_drop_rate, attn_drop=attn_drop_rate, proj_drop=proj_drop_rate, drop_path_prob=drop_path_rate, \n",
    "                                 name='name{}'.format(i))(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def swin_unet_2d_base(input_tensor, filter_num_begin, depth, stack_num_down, stack_num_up, \n",
    "                      patch_size, num_heads, window_size, num_mlp, shift_window=True, name='swin_unet'):\n",
    "   \n",
    "    # Compute number be patches to be embeded\n",
    "    input_size = input_tensor.shape.as_list()[1:]\n",
    "    num_patch_x = input_size[0]//patch_size[0]\n",
    "    num_patch_y = input_size[1]//patch_size[1]\n",
    "    \n",
    "    # Number of Embedded dimensions\n",
    "    embed_dim = filter_num_begin\n",
    "    \n",
    "    depth_ = depth\n",
    "    \n",
    "    X_skip = []\n",
    "\n",
    "    X = input_tensor\n",
    "    \n",
    "    # Patch extraction\n",
    "    X = patch_extract(patch_size)(X)\n",
    "\n",
    "    # Embed patches to tokens\n",
    "    X = patch_embedding(num_patch_x*num_patch_y, embed_dim)(X)\n",
    "    \n",
    "    # The first Swin Transformer stack\n",
    "    X = swin_transformer_stack(X, stack_num=stack_num_down, \n",
    "                               embed_dim=embed_dim, num_patch=(num_patch_x, num_patch_y), \n",
    "                               num_heads=num_heads[0], window_size=window_size[0], num_mlp=num_mlp, \n",
    "                               shift_window=shift_window, name='{}_swin_down0'.format(name))\n",
    "    X_skip.append(X)\n",
    "    \n",
    "    # Downsampling blocks\n",
    "    for i in range(depth_-1):\n",
    "        \n",
    "        # Patch merging\n",
    "        X = patch_merging((num_patch_x, num_patch_y), embed_dim=embed_dim, name='down{}'.format(i))(X)\n",
    "        \n",
    "        # update token shape info\n",
    "        embed_dim = embed_dim*2\n",
    "        num_patch_x = num_patch_x//2\n",
    "        num_patch_y = num_patch_y//2\n",
    "        \n",
    "        # Swin Transformer stacks\n",
    "        X = swin_transformer_stack(X, stack_num=stack_num_down, \n",
    "                                   embed_dim=embed_dim, num_patch=(num_patch_x, num_patch_y), \n",
    "                                   num_heads=num_heads[i+1], window_size=window_size[i+1], num_mlp=num_mlp, \n",
    "                                   shift_window=shift_window, name='{}_swin_down{}'.format(name, i+1))\n",
    "        \n",
    "        # Store tensors for concat\n",
    "        X_skip.append(X)\n",
    "        \n",
    "    # reverse indexing encoded tensors and hyperparams\n",
    "    X_skip = X_skip[::-1]\n",
    "    num_heads = num_heads[::-1]\n",
    "    window_size = window_size[::-1]\n",
    "    \n",
    "    # upsampling begins at the deepest available tensor\n",
    "    X = X_skip[0]\n",
    "    \n",
    "    # other tensors are preserved for concatenation\n",
    "    X_decode = X_skip[1:]\n",
    "    \n",
    "    depth_decode = len(X_decode)\n",
    "    \n",
    "    for i in range(depth_decode):\n",
    "        \n",
    "        # Patch expanding\n",
    "        X = patch_expanding(num_patch=(num_patch_x, num_patch_y),\n",
    "                            embed_dim=embed_dim, upsample_rate=2, return_vector=True, name='{}_swin_up{}'.format(name, i))(X)\n",
    "        \n",
    "\n",
    "        # update token shape info\n",
    "        embed_dim = embed_dim//2\n",
    "        num_patch_x = num_patch_x*2\n",
    "        num_patch_y = num_patch_y*2\n",
    "        \n",
    "        # Concatenation and linear projection\n",
    "        X = concatenate([X, X_decode[i]], axis=-1, name='{}_concat_{}'.format(name, i))\n",
    "        X = Dense(embed_dim, use_bias=False, name='{}_concat_linear_proj_{}'.format(name, i))(X)\n",
    "        \n",
    "        # Swin Transformer stacks\n",
    "        X = swin_transformer_stack(X, stack_num=stack_num_up, \n",
    "                           embed_dim=embed_dim, num_patch=(num_patch_x, num_patch_y), \n",
    "                           num_heads=num_heads[i], window_size=window_size[i], num_mlp=num_mlp, \n",
    "                           shift_window=shift_window, name='{}_swin_up{}'.format(name, i))\n",
    "        \n",
    "    # The last expanding layer; it produces full-size feature maps based on the patch size\n",
    "    # !!! <--- \"patch_size[0]\" is used; it assumes patch_size = (size, size)\n",
    "    X = patch_expanding(num_patch=(num_patch_x, num_patch_y),\n",
    "                        embed_dim=embed_dim, upsample_rate=patch_size[0], return_vector=False)(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# def create_mask(input_tensor):\n",
    "#         input_size = input_tensor.shape.as_list()[1:]\n",
    "# #         all_data_tensor = tensor.empty((0, input_tensor.shape[0]), dtype=torch.float32)\n",
    "        \n",
    "# #         print()\n",
    "#         newi = input_tensor.shape[0]\n",
    "#         if(newi == None):\n",
    "#             newi = 1\n",
    "# #         new_label = tf.zeros((newi,512,512,3))\n",
    "#         input_tensor1  = Input(shape=(newi,512,512,3))\n",
    "#         new_label = K.get_value(input_tensor1)\n",
    "#         for i in range(newi):\n",
    "#             for j in range(input_tensor.shape[1]):\n",
    "#                 for k in range(input_tensor.shape[2]):\n",
    "#                     index = tf.math.argmax(input_tensor[i][j][k])\n",
    "#                     tf.keras.backend.set_value(new_label[i][j][k][index-1], 1)\n",
    "                    \n",
    "# #                     new_label[i][j][index-1].assign(1)\n",
    "#                     break\n",
    "# #                     print(tf.get_static_value(input_tensor[i][j][k]))\n",
    "# #                     print(input_tensor[i][j][k].print())\n",
    "# #                     print(tf.math.argmax(input_tensor[i][j][k]))\n",
    "#                 break\n",
    "#             break\n",
    "        \n",
    "# #                 index = np.argmax(input_tensor[i][j])\n",
    "# #                 if(index>0):\n",
    "# #                     new_label[i][j][index-1]=255\n",
    "#         K.set_value(input_tensor1, new_label)\n",
    "#         return input_tensor1\n",
    "\n",
    "def swin_unet_2d(input_size, filter_num_begin, n_labels, depth, stack_num_down, stack_num_up, \n",
    "                      patch_size, num_heads, window_size, num_mlp, output_activation='Softmax', shift_window=True, name='swin_unet'):\n",
    "    \n",
    "    IN = Input(input_size)\n",
    "    \n",
    "    # base    \n",
    "    X = swin_unet_2d_base(IN, filter_num_begin=filter_num_begin, depth=depth, stack_num_down=stack_num_down, stack_num_up=stack_num_up, \n",
    "                          patch_size=patch_size, num_heads=num_heads, window_size=window_size, num_mlp=num_mlp, shift_window=shift_window, name=name)\n",
    "    \n",
    "    # output layer\n",
    "    OUT = CONV_output(X, n_labels, kernel_size=1, activation=output_activation, name='{}_output'.format(name))\n",
    "#   \n",
    "#     NEW_OUT = create_mask(OUT)\n",
    "#     concatLayer = tf.keras.layers.Concatenate()([IN, OUT])\n",
    "#     new_X = swin_unet_2d_base(concatLayer, filter_num_begin=filter_num_begin, depth=depth, stack_num_down=stack_num_down, stack_num_up=stack_num_up, \n",
    "#                           patch_size=patch_size, num_heads=num_heads, window_size=window_size, num_mlp=num_mlp, shift_window=shift_window, name=name+\"second\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     OUT2 = CONV_output(new_X, n_labels, kernel_size=1, activation=output_activation, name='{}_output'.format(name+\"second\"))\n",
    "    \n",
    "    \n",
    "    # functional API model\n",
    "    model = Model(inputs=[IN,], outputs=[OUT,], name='{}_model'.format(name))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model_alias = f\"1_swin_unet_normal\"\n",
    "# model = swin_unet_2d( (512,512,3) , filter_num_begin=16, n_labels=2, \n",
    "#                                     depth=3, stack_num_down=2, stack_num_up=2, patch_size=(2, 2), \n",
    "#                                     num_heads=[4, 8, 8, 8], window_size=[4, 2, 2, 2], num_mlp=128, \n",
    "#                                 output_activation='Softmax', shift_window=True, name=model_alias)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Swin WNet Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T09:20:13.236478Z",
     "iopub.status.busy": "2024-09-22T09:20:13.236197Z",
     "iopub.status.idle": "2024-09-22T09:20:13.247122Z",
     "shell.execute_reply": "2024-09-22T09:20:13.246382Z",
     "shell.execute_reply.started": "2024-09-22T09:20:13.236453Z"
    }
   },
   "outputs": [],
   "source": [
    "def swin_wnet_2d(input_size, filter_num_begin, n_labels1,n_labels2, depth, stack_num_down, stack_num_up, \n",
    "                      patch_size, num_heads, window_size, num_mlp, output_activation='Softmax', shift_window=True, name='swin_unet'):\n",
    "\n",
    "    IN = Input(input_size)\n",
    "    \n",
    "    # base    \n",
    "    X = swin_unet_2d_base(IN, filter_num_begin=filter_num_begin, depth=depth, stack_num_down=stack_num_down, stack_num_up=stack_num_up, \n",
    "                          patch_size=patch_size, num_heads=num_heads, window_size=window_size, num_mlp=num_mlp, shift_window=shift_window, name=name)\n",
    "    \n",
    "    # output layer\n",
    "    OUT = CONV_output(X, n_labels1, kernel_size=1, activation=output_activation, name='{}_output'.format(name))\n",
    "#   \n",
    "    concatLayer = tf.keras.layers.Concatenate()([IN[:,:,:,0:3], OUT])\n",
    "    new_X = swin_unet_2d_base(concatLayer, filter_num_begin=filter_num_begin, depth=depth, stack_num_down=stack_num_down, stack_num_up=stack_num_up, \n",
    "                          patch_size=patch_size, num_heads=num_heads, window_size=window_size, num_mlp=num_mlp, shift_window=shift_window, name=name+\"second\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    OUT2 = CONV_output(new_X, n_labels2, kernel_size=1, activation=output_activation, name='{}_output'.format(name+\"second\"))\n",
    "    \n",
    "    \n",
    "    # functional API model\n",
    "    model = Model(inputs=[IN,], outputs=[OUT2,], name='{}_model'.format(name+\"second\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model_alias = f\"1_swin_Wnet_normal\"\n",
    "# model = swin_wnet_2d( (512,512,3) , filter_num_begin=16, n_labels1=2, n_labels2=4,\n",
    "#                                     depth=3, stack_num_down=2, stack_num_up=2, patch_size=(2, 2), \n",
    "#                                     num_heads=[4, 8, 8, 8], window_size=[4, 2, 2, 2], num_mlp=128, \n",
    "#                                 output_activation='Softmax', shift_window=True, name=model_alias)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Focal Loss Definition, Model Checkpoint, Optimizer and Callback definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T09:20:13.248365Z",
     "iopub.status.busy": "2024-09-22T09:20:13.248114Z",
     "iopub.status.idle": "2024-09-22T09:20:13.258479Z",
     "shell.execute_reply": "2024-09-22T09:20:13.257726Z",
     "shell.execute_reply.started": "2024-09-22T09:20:13.248334Z"
    },
    "id": "Wzzka7V0qly8"
   },
   "outputs": [],
   "source": [
    "\n",
    "def focal_weighted(y_true,y_pred):\n",
    "    GAMMA_FOUR_CLASS = [ 2, 2, 2, 2]\n",
    "    POS_WEIGHT_FOUR_CLASS = [ 0.1, 0.8, 0.9, 0.8]\n",
    "\n",
    "    loss = 0\n",
    "    w = 2\n",
    "    gamma = 2\n",
    "    edge_gamma = 2\n",
    "    # loss_func = BinaryFocalLoss(gamma=GAMMA_FOUR_CLASS[0],pos_weight = POS_WEIGHT_FOUR_CLASS[0])\n",
    "    # loss = loss + loss_func(y_true[:,:,:,0],y_pred[:,:,:,0])\n",
    "\n",
    "    loss_func = BinaryFocalLoss(gamma=GAMMA_FOUR_CLASS[1],pos_weight = POS_WEIGHT_FOUR_CLASS[1])\n",
    "    loss = loss + loss_func(y_true[:,:,:,1],y_pred[:,:,:,1])\n",
    "\n",
    "    loss_func = BinaryFocalLoss(gamma=GAMMA_FOUR_CLASS[2],pos_weight = POS_WEIGHT_FOUR_CLASS[2])\n",
    "    loss = loss + loss_func(y_true[:,:,:,2],y_pred[:,:,:,2])\n",
    "\n",
    "    loss_func = BinaryFocalLoss(gamma=GAMMA_FOUR_CLASS[3],pos_weight = POS_WEIGHT_FOUR_CLASS[3])\n",
    "    loss = loss + loss_func(y_true[:,:,:,3],y_pred[:,:,:,3])\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T09:20:13.259852Z",
     "iopub.status.busy": "2024-09-22T09:20:13.259516Z",
     "iopub.status.idle": "2024-09-22T09:20:13.271869Z",
     "shell.execute_reply": "2024-09-22T09:20:13.270955Z",
     "shell.execute_reply.started": "2024-09-22T09:20:13.259820Z"
    },
    "id": "1P-jawCTqfM6"
   },
   "outputs": [],
   "source": [
    "#@title Definitions - DisplayCallback()\n",
    "class DisplayCallback(keras.callbacks.Callback):\n",
    "    def __init__(self,model, train_images,train_labels, epoch_interval=5):\n",
    "        self.model = model\n",
    "        self.train = train_images\n",
    "        self.label = train_labels\n",
    "        self.epoch_interval = epoch_interval\n",
    "    \n",
    "    def display(self, display_list, extra_title=''):\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "        if len(display_list) > len(title):\n",
    "            title.append(extra_title)\n",
    "\n",
    "        for i in range(len(display_list)):\n",
    "            plt.subplot(1, len(display_list), i+1)\n",
    "            plt.title(title[i])\n",
    "            plt.imshow(display_list[i], cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "    def create_mask(self, pred_mask):\n",
    "        new_label = np.zeros((Image_Size,Image_Size,3))\n",
    "        for i in range(Image_Size):\n",
    "            for j in range(Image_Size):\n",
    "                index = np.argmax(pred_mask[i][j])\n",
    "                if(index>0):\n",
    "                    new_label[i][j][index-1]=255\n",
    "        return new_label\n",
    "    \n",
    "    def show_predictions(self, train,label, num=1):\n",
    "#         index = random.randint(0, 9)\n",
    "        pred_mask = self.model.predict(train)\n",
    "        print(type(pred_mask))\n",
    "        print(pred_mask.shape)\n",
    "\n",
    "        self.display([train[0], self.create_mask(label[0]), self.create_mask(pred_mask[0])])\n",
    "#         for image, mask in dataset.take(num):\n",
    "#             pred_mask = model.predict(image)\n",
    "#             self.display([image[0], mask[0], self.create_mask(pred_mask)])\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch and epoch % self.epoch_interval == 0:\n",
    "            self.show_predictions(self.train, self.label)\n",
    "            print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T09:20:13.273103Z",
     "iopub.status.busy": "2024-09-22T09:20:13.272836Z",
     "iopub.status.idle": "2024-09-22T09:20:13.285768Z",
     "shell.execute_reply": "2024-09-22T09:20:13.285179Z",
     "shell.execute_reply.started": "2024-09-22T09:20:13.273080Z"
    },
    "id": "0Q3Xy0oVpw9l"
   },
   "outputs": [],
   "source": [
    "#@title Definitions - generate_callbacks(), generate_loss(), generate_optimizer()\n",
    "\n",
    "def generate_callbacks(model, CHECKPOINT_MODE, CHECKPOINT_PATH, CHECKPOINT_MONITOR, CALL_BACK_INTERVAL,SampleTestImage, SampleTestLabel):\n",
    "    checkpoint = ModelCheckpoint(CHECKPOINT_PATH, monitor=CHECKPOINT_MONITOR, verbose=1, save_best_only=True, mode=CHECKPOINT_MODE)\n",
    "    earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200, mode=\"min\")\n",
    "    callbacks_list = [DisplayCallback(model, SampleTestImage, SampleTestLabel, CALL_BACK_INTERVAL), checkpoint, earlyStopping]\n",
    "    \n",
    "    return callbacks_list\n",
    "\n",
    "def generate_loss(loss_function):\n",
    "    if(loss_function == \"sparse\"):\n",
    "        loss = \"sparse_categorical_crossentropy\"\n",
    "    elif(loss_function == \"catcross\"):\n",
    "        loss = \"categorical_crossentropy\"\n",
    "    elif(loss_function == \"focal_loss\"):\n",
    "        loss = focal_weighted\n",
    "    return loss\n",
    "        \n",
    "\n",
    "def generate_optimizer(optimizer, learning_rate):\n",
    "    if(optimizer == \"Adam\"):\n",
    "        opt = tf.optimizers.Adam(learning_rate)\n",
    "        return opt\n",
    "    if(optimizer == \"Lamb\"):\n",
    "        opt = tfa.optimizers.LAMB(learning_rate = learning_rate, weight_decay = 2*(1e-2) )\n",
    "        return opt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train and Test Data Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T09:20:13.287677Z",
     "iopub.status.busy": "2024-09-22T09:20:13.287041Z",
     "iopub.status.idle": "2024-09-22T09:20:13.296267Z",
     "shell.execute_reply": "2024-09-22T09:20:13.295400Z",
     "shell.execute_reply.started": "2024-09-22T09:20:13.287652Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def green_channel_clahe_zscore(images):\n",
    "    # Initialize an empty array to store the processed images\n",
    "    processed_images = []\n",
    "\n",
    "    # Loop through the input images\n",
    "    for img in images:\n",
    "        # Extract the green channel\n",
    "        green_channel = img[:, :, 1]\n",
    "\n",
    "        # Apply CLAHE to enhance contrast\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        green_clahe = clahe.apply(green_channel)\n",
    "\n",
    "        # Z-score normalization\n",
    "        mean = np.mean(green_clahe)\n",
    "        std = np.std(green_clahe)\n",
    "        green_zscore = (green_clahe - mean) / std\n",
    "\n",
    "        # Append the processed image to the output array\n",
    "        processed_images.append(green_zscore)\n",
    "\n",
    "    # Convert the output array to a numpy array\n",
    "    processed_images = np.array(processed_images)\n",
    "    processed_images = np.expand_dims(processed_images, axis=-1)\n",
    "\n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T09:21:19.491703Z",
     "iopub.status.busy": "2024-09-22T09:21:19.491014Z",
     "iopub.status.idle": "2024-09-22T09:21:19.916903Z",
     "shell.execute_reply": "2024-09-22T09:21:19.916001Z",
     "shell.execute_reply.started": "2024-09-22T09:21:19.491668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 512, 512, 1) (6, 512, 512, 4)\n"
     ]
    }
   ],
   "source": [
    "ChannelType = \"green\"\n",
    "# if(Dataset == \"DRIVE_AV\"):\n",
    "#     train_generator = MyDataGenerator(Train_Image_Path, Train_Label_Path, ChannelType, batch_size=30, shuffle = False)\n",
    "#     test_generator = MyDataGenerator(Test_Image_Path, Test_Label_Path, ChannelType, batch_size=30, shuffle = False)\n",
    "# elif(Dataset == \"HRF_AV\"):\n",
    "#     test_generator = MyDataGenerator(Train_Image_Path, Train_Label_Path, ChannelType, batch_size=30, shuffle = False)\n",
    "#     train_generator = MyDataGenerator(Test_Image_Path, Test_Label_Path, ChannelType, batch_size=30, shuffle = False)\n",
    "\n",
    "# TrainX, TrainY = train_generator.__getitem__(0)\n",
    "test_generator = MyDataGenerator(Test_Image_Path, Test_Label_Path, ChannelType, batch_size=6, shuffle = False)\n",
    "TestX, TestY = test_generator.__getitem__(0)\n",
    "\n",
    "print(TestX.shape, TestY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T09:21:22.978771Z",
     "iopub.status.busy": "2024-09-22T09:21:22.978398Z",
     "iopub.status.idle": "2024-09-22T09:21:30.736275Z",
     "shell.execute_reply": "2024-09-22T09:21:30.735429Z",
     "shell.execute_reply.started": "2024-09-22T09:21:22.978743Z"
    }
   },
   "outputs": [],
   "source": [
    "TrainX = green_channel_clahe_zscore(RawTrainX)\n",
    "TrainY = test_generator.__convertlabelto4channel__(RawTrainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T09:21:34.142240Z",
     "iopub.status.busy": "2024-09-22T09:21:34.141870Z",
     "iopub.status.idle": "2024-09-22T09:21:34.146956Z",
     "shell.execute_reply": "2024-09-22T09:21:34.145902Z",
     "shell.execute_reply.started": "2024-09-22T09:21:34.142213Z"
    }
   },
   "outputs": [],
   "source": [
    "no_epoch = 200\n",
    "n_labels = 4\n",
    "optimizers = \"Adam\"\n",
    "losses = \"focal_loss\"\n",
    "batch_sizes = 6\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Initiate and Train Model** \n",
    "### **Note:** Run either one of the cell, i.e Attention-WNet and Swin-WNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T09:21:36.271953Z",
     "iopub.status.busy": "2024-09-22T09:21:36.271105Z",
     "iopub.status.idle": "2024-09-22T09:21:40.695010Z",
     "shell.execute_reply": "2024-09-22T09:21:40.694138Z",
     "shell.execute_reply.started": "2024-09-22T09:21:36.271920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"attwnet_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 512, 512, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " attwnet_down0_0 (Conv2D)       (None, 512, 512, 32  288         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " attwnet_down0_0_bn (BatchNorma  (None, 512, 512, 32  128        ['attwnet_down0_0[0][0]']        \n",
      " lization)                      )                                                                 \n",
      "                                                                                                  \n",
      " attwnet_down0_0_activation (Re  (None, 512, 512, 32  0          ['attwnet_down0_0_bn[0][0]']     \n",
      " LU)                            )                                                                 \n",
      "                                                                                                  \n",
      " attwnet_down0_1 (Conv2D)       (None, 512, 512, 32  9216        ['attwnet_down0_0_activation[0][0\n",
      "                                )                                ]']                              \n",
      "                                                                                                  \n",
      " attwnet_down0_1_bn (BatchNorma  (None, 512, 512, 32  128        ['attwnet_down0_1[0][0]']        \n",
      " lization)                      )                                                                 \n",
      "                                                                                                  \n",
      " attwnet_down0_1_activation (Re  (None, 512, 512, 32  0          ['attwnet_down0_1_bn[0][0]']     \n",
      " LU)                            )                                                                 \n",
      "                                                                                                  \n",
      " attwnet_down1_encode_maxpool (  (None, 256, 256, 32  0          ['attwnet_down0_1_activation[0][0\n",
      " MaxPooling2D)                  )                                ]']                              \n",
      "                                                                                                  \n",
      " attwnet_down1_conv_0 (Conv2D)  (None, 256, 256, 64  18432       ['attwnet_down1_encode_maxpool[0]\n",
      "                                )                                [0]']                            \n",
      "                                                                                                  \n",
      " attwnet_down1_conv_0_bn (Batch  (None, 256, 256, 64  256        ['attwnet_down1_conv_0[0][0]']   \n",
      " Normalization)                 )                                                                 \n",
      "                                                                                                  \n",
      " attwnet_down1_conv_0_activatio  (None, 256, 256, 64  0          ['attwnet_down1_conv_0_bn[0][0]']\n",
      " n (ReLU)                       )                                                                 \n",
      "                                                                                                  \n",
      " attwnet_down1_conv_1 (Conv2D)  (None, 256, 256, 64  36864       ['attwnet_down1_conv_0_activation\n",
      "                                )                                [0][0]']                         \n",
      "                                                                                                  \n",
      " attwnet_down1_conv_1_bn (Batch  (None, 256, 256, 64  256        ['attwnet_down1_conv_1[0][0]']   \n",
      " Normalization)                 )                                                                 \n",
      "                                                                                                  \n",
      " attwnet_down1_conv_1_activatio  (None, 256, 256, 64  0          ['attwnet_down1_conv_1_bn[0][0]']\n",
      " n (ReLU)                       )                                                                 \n",
      "                                                                                                  \n",
      " attwnet_down2_encode_maxpool (  (None, 128, 128, 64  0          ['attwnet_down1_conv_1_activation\n",
      " MaxPooling2D)                  )                                [0][0]']                         \n",
      "                                                                                                  \n",
      " attwnet_down2_conv_0 (Conv2D)  (None, 128, 128, 12  73728       ['attwnet_down2_encode_maxpool[0]\n",
      "                                8)                               [0]']                            \n",
      "                                                                                                  \n",
      " attwnet_down2_conv_0_bn (Batch  (None, 128, 128, 12  512        ['attwnet_down2_conv_0[0][0]']   \n",
      " Normalization)                 8)                                                                \n",
      "                                                                                                  \n",
      " attwnet_down2_conv_0_activatio  (None, 128, 128, 12  0          ['attwnet_down2_conv_0_bn[0][0]']\n",
      " n (ReLU)                       8)                                                                \n",
      "                                                                                                  \n",
      " attwnet_down2_conv_1 (Conv2D)  (None, 128, 128, 12  147456      ['attwnet_down2_conv_0_activation\n",
      "                                8)                               [0][0]']                         \n",
      "                                                                                                  \n",
      " attwnet_down2_conv_1_bn (Batch  (None, 128, 128, 12  512        ['attwnet_down2_conv_1[0][0]']   \n",
      " Normalization)                 8)                                                                \n",
      "                                                                                                  \n",
      " attwnet_down2_conv_1_activatio  (None, 128, 128, 12  0          ['attwnet_down2_conv_1_bn[0][0]']\n",
      " n (ReLU)                       8)                                                                \n",
      "                                                                                                  \n",
      " attwnet_down3_encode_maxpool (  (None, 64, 64, 128)  0          ['attwnet_down2_conv_1_activation\n",
      " MaxPooling2D)                                                   [0][0]']                         \n",
      "                                                                                                  \n",
      " attwnet_down3_conv_0 (Conv2D)  (None, 64, 64, 256)  294912      ['attwnet_down3_encode_maxpool[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " attwnet_down3_conv_0_bn (Batch  (None, 64, 64, 256)  1024       ['attwnet_down3_conv_0[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " attwnet_down3_conv_0_activatio  (None, 64, 64, 256)  0          ['attwnet_down3_conv_0_bn[0][0]']\n",
      " n (ReLU)                                                                                         \n",
      "                                                                                                  \n",
      " attwnet_down3_conv_1 (Conv2D)  (None, 64, 64, 256)  589824      ['attwnet_down3_conv_0_activation\n",
      "                                                                 [0][0]']                         \n",
      "                                                                                                  \n",
      " attwnet_down3_conv_1_bn (Batch  (None, 64, 64, 256)  1024       ['attwnet_down3_conv_1[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " attwnet_down3_conv_1_activatio  (None, 64, 64, 256)  0          ['attwnet_down3_conv_1_bn[0][0]']\n",
      " n (ReLU)                                                                                         \n",
      "                                                                                                  \n",
      " attwnet_up0_decode_trans_conv   (None, 128, 128, 12  295040     ['attwnet_down3_conv_1_activation\n",
      " (Conv2DTranspose)              8)                               [0][0]']                         \n",
      "                                                                                                  \n",
      " attwnet_up0_decode_bn (BatchNo  (None, 128, 128, 12  512        ['attwnet_up0_decode_trans_conv[0\n",
      " rmalization)                   8)                               ][0]']                           \n",
      "                                                                                                  \n",
      " attwnet_up0_decode_activation   (None, 128, 128, 12  0          ['attwnet_up0_decode_bn[0][0]']  \n",
      " (ReLU)                         8)                                                                \n",
      "                                                                                                  \n",
      " attwnet_up0_att_theta_x (Conv2  (None, 128, 128, 64  8256       ['attwnet_down2_conv_1_activation\n",
      " D)                             )                                [0][0]']                         \n",
      "                                                                                                  \n",
      " attwnet_up0_att_phi_g (Conv2D)  (None, 128, 128, 64  8256       ['attwnet_up0_decode_activation[0\n",
      "                                )                                ][0]']                           \n",
      "                                                                                                  \n",
      " attwnet_up0_att_add (Add)      (None, 128, 128, 64  0           ['attwnet_up0_att_theta_x[0][0]',\n",
      "                                )                                 'attwnet_up0_att_phi_g[0][0]']  \n",
      "                                                                                                  \n",
      " attwnet_up0_att_activation (Re  (None, 128, 128, 64  0          ['attwnet_up0_att_add[0][0]']    \n",
      " LU)                            )                                                                 \n",
      "                                                                                                  \n",
      " attwnet_up0_att_psi_f (Conv2D)  (None, 128, 128, 1)  65         ['attwnet_up0_att_activation[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " attwnet_up0_att_sigmoid (Activ  (None, 128, 128, 1)  0          ['attwnet_up0_att_psi_f[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " attwnet_up0_att_masking (Multi  (None, 128, 128, 12  0          ['attwnet_down2_conv_1_activation\n",
      " ply)                           8)                               [0][0]',                         \n",
      "                                                                  'attwnet_up0_att_sigmoid[0][0]']\n",
      "                                                                                                  \n",
      " attwnet_up0_concat (Concatenat  (None, 128, 128, 25  0          ['attwnet_up0_decode_activation[0\n",
      " e)                             6)                               ][0]',                           \n",
      "                                                                  'attwnet_up0_att_masking[0][0]']\n",
      "                                                                                                  \n",
      " attwnet_up0_conv_after_concat_  (None, 128, 128, 12  294912     ['attwnet_up0_concat[0][0]']     \n",
      " 0 (Conv2D)                     8)                                                                \n",
      "                                                                                                  \n",
      " attwnet_up0_conv_after_concat_  (None, 128, 128, 12  512        ['attwnet_up0_conv_after_concat_0\n",
      " 0_bn (BatchNormalization)      8)                               [0][0]']                         \n",
      "                                                                                                  \n",
      " attwnet_up0_conv_after_concat_  (None, 128, 128, 12  0          ['attwnet_up0_conv_after_concat_0\n",
      " 0_activation (ReLU)            8)                               _bn[0][0]']                      \n",
      "                                                                                                  \n",
      " attwnet_up0_conv_after_concat_  (None, 128, 128, 12  147456     ['attwnet_up0_conv_after_concat_0\n",
      " 1 (Conv2D)                     8)                               _activation[0][0]']              \n",
      "                                                                                                  \n",
      " attwnet_up0_conv_after_concat_  (None, 128, 128, 12  512        ['attwnet_up0_conv_after_concat_1\n",
      " 1_bn (BatchNormalization)      8)                               [0][0]']                         \n",
      "                                                                                                  \n",
      " attwnet_up0_conv_after_concat_  (None, 128, 128, 12  0          ['attwnet_up0_conv_after_concat_1\n",
      " 1_activation (ReLU)            8)                               _bn[0][0]']                      \n",
      "                                                                                                  \n",
      " attwnet_up1_decode_trans_conv   (None, 256, 256, 64  73792      ['attwnet_up0_conv_after_concat_1\n",
      " (Conv2DTranspose)              )                                _activation[0][0]']              \n",
      "                                                                                                  \n",
      " attwnet_up1_decode_bn (BatchNo  (None, 256, 256, 64  256        ['attwnet_up1_decode_trans_conv[0\n",
      " rmalization)                   )                                ][0]']                           \n",
      "                                                                                                  \n",
      " attwnet_up1_decode_activation   (None, 256, 256, 64  0          ['attwnet_up1_decode_bn[0][0]']  \n",
      " (ReLU)                         )                                                                 \n",
      "                                                                                                  \n",
      " attwnet_up1_att_theta_x (Conv2  (None, 256, 256, 32  2080       ['attwnet_down1_conv_1_activation\n",
      " D)                             )                                [0][0]']                         \n",
      "                                                                                                  \n",
      " attwnet_up1_att_phi_g (Conv2D)  (None, 256, 256, 32  2080       ['attwnet_up1_decode_activation[0\n",
      "                                )                                ][0]']                           \n",
      "                                                                                                  \n",
      " attwnet_up1_att_add (Add)      (None, 256, 256, 32  0           ['attwnet_up1_att_theta_x[0][0]',\n",
      "                                )                                 'attwnet_up1_att_phi_g[0][0]']  \n",
      "                                                                                                  \n",
      " attwnet_up1_att_activation (Re  (None, 256, 256, 32  0          ['attwnet_up1_att_add[0][0]']    \n",
      " LU)                            )                                                                 \n",
      "                                                                                                  \n",
      " attwnet_up1_att_psi_f (Conv2D)  (None, 256, 256, 1)  33         ['attwnet_up1_att_activation[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " attwnet_up1_att_sigmoid (Activ  (None, 256, 256, 1)  0          ['attwnet_up1_att_psi_f[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " attwnet_up1_att_masking (Multi  (None, 256, 256, 64  0          ['attwnet_down1_conv_1_activation\n",
      " ply)                           )                                [0][0]',                         \n",
      "                                                                  'attwnet_up1_att_sigmoid[0][0]']\n",
      "                                                                                                  \n",
      " attwnet_up1_concat (Concatenat  (None, 256, 256, 12  0          ['attwnet_up1_decode_activation[0\n",
      " e)                             8)                               ][0]',                           \n",
      "                                                                  'attwnet_up1_att_masking[0][0]']\n",
      "                                                                                                  \n",
      " attwnet_up1_conv_after_concat_  (None, 256, 256, 64  73728      ['attwnet_up1_concat[0][0]']     \n",
      " 0 (Conv2D)                     )                                                                 \n",
      "                                                                                                  \n",
      " attwnet_up1_conv_after_concat_  (None, 256, 256, 64  256        ['attwnet_up1_conv_after_concat_0\n",
      " 0_bn (BatchNormalization)      )                                [0][0]']                         \n",
      "                                                                                                  \n",
      " attwnet_up1_conv_after_concat_  (None, 256, 256, 64  0          ['attwnet_up1_conv_after_concat_0\n",
      " 0_activation (ReLU)            )                                _bn[0][0]']                      \n",
      "                                                                                                  \n",
      " attwnet_up1_conv_after_concat_  (None, 256, 256, 64  36864      ['attwnet_up1_conv_after_concat_0\n",
      " 1 (Conv2D)                     )                                _activation[0][0]']              \n",
      "                                                                                                  \n",
      " attwnet_up1_conv_after_concat_  (None, 256, 256, 64  256        ['attwnet_up1_conv_after_concat_1\n",
      " 1_bn (BatchNormalization)      )                                [0][0]']                         \n",
      "                                                                                                  \n",
      " attwnet_up1_conv_after_concat_  (None, 256, 256, 64  0          ['attwnet_up1_conv_after_concat_1\n",
      " 1_activation (ReLU)            )                                _bn[0][0]']                      \n",
      "                                                                                                  \n",
      " attwnet_up2_decode_trans_conv   (None, 512, 512, 32  18464      ['attwnet_up1_conv_after_concat_1\n",
      " (Conv2DTranspose)              )                                _activation[0][0]']              \n",
      "                                                                                                  \n",
      " attwnet_up2_decode_bn (BatchNo  (None, 512, 512, 32  128        ['attwnet_up2_decode_trans_conv[0\n",
      " rmalization)                   )                                ][0]']                           \n",
      "                                                                                                  \n",
      " attwnet_up2_decode_activation   (None, 512, 512, 32  0          ['attwnet_up2_decode_bn[0][0]']  \n",
      " (ReLU)                         )                                                                 \n",
      "                                                                                                  \n",
      " attwnet_up2_att_theta_x (Conv2  (None, 512, 512, 16  528        ['attwnet_down0_1_activation[0][0\n",
      " D)                             )                                ]']                              \n",
      "                                                                                                  \n",
      " attwnet_up2_att_phi_g (Conv2D)  (None, 512, 512, 16  528        ['attwnet_up2_decode_activation[0\n",
      "                                )                                ][0]']                           \n",
      "                                                                                                  \n",
      " attwnet_up2_att_add (Add)      (None, 512, 512, 16  0           ['attwnet_up2_att_theta_x[0][0]',\n",
      "                                )                                 'attwnet_up2_att_phi_g[0][0]']  \n",
      "                                                                                                  \n",
      " attwnet_up2_att_activation (Re  (None, 512, 512, 16  0          ['attwnet_up2_att_add[0][0]']    \n",
      " LU)                            )                                                                 \n",
      "                                                                                                  \n",
      " attwnet_up2_att_psi_f (Conv2D)  (None, 512, 512, 1)  17         ['attwnet_up2_att_activation[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " attwnet_up2_att_sigmoid (Activ  (None, 512, 512, 1)  0          ['attwnet_up2_att_psi_f[0][0]']  \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " attwnet_up2_att_masking (Multi  (None, 512, 512, 32  0          ['attwnet_down0_1_activation[0][0\n",
      " ply)                           )                                ]',                              \n",
      "                                                                  'attwnet_up2_att_sigmoid[0][0]']\n",
      "                                                                                                  \n",
      " attwnet_up2_concat (Concatenat  (None, 512, 512, 64  0          ['attwnet_up2_decode_activation[0\n",
      " e)                             )                                ][0]',                           \n",
      "                                                                  'attwnet_up2_att_masking[0][0]']\n",
      "                                                                                                  \n",
      " attwnet_up2_conv_after_concat_  (None, 512, 512, 32  18432      ['attwnet_up2_concat[0][0]']     \n",
      " 0 (Conv2D)                     )                                                                 \n",
      "                                                                                                  \n",
      " attwnet_up2_conv_after_concat_  (None, 512, 512, 32  128        ['attwnet_up2_conv_after_concat_0\n",
      " 0_bn (BatchNormalization)      )                                [0][0]']                         \n",
      "                                                                                                  \n",
      " attwnet_up2_conv_after_concat_  (None, 512, 512, 32  0          ['attwnet_up2_conv_after_concat_0\n",
      " 0_activation (ReLU)            )                                _bn[0][0]']                      \n",
      "                                                                                                  \n",
      " attwnet_up2_conv_after_concat_  (None, 512, 512, 32  9216       ['attwnet_up2_conv_after_concat_0\n",
      " 1 (Conv2D)                     )                                _activation[0][0]']              \n",
      "                                                                                                  \n",
      " attwnet_up2_conv_after_concat_  (None, 512, 512, 32  128        ['attwnet_up2_conv_after_concat_1\n",
      " 1_bn (BatchNormalization)      )                                [0][0]']                         \n",
      "                                                                                                  \n",
      " attwnet_up2_conv_after_concat_  (None, 512, 512, 32  0          ['attwnet_up2_conv_after_concat_1\n",
      " 1_activation (ReLU)            )                                _bn[0][0]']                      \n",
      "                                                                                                  \n",
      " attwnet_output (Conv2D)        (None, 512, 512, 4)  132         ['attwnet_up2_conv_after_concat_1\n",
      "                                                                 _activation[0][0]']              \n",
      "                                                                                                  \n",
      " attwnet_output_activation (Sof  (None, 512, 512, 4)  0          ['attwnet_output[0][0]']         \n",
      " tmax)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 512, 512, 5)  0           ['input_1[0][0]',                \n",
      "                                                                  'attwnet_output_activation[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " attwnetsecond_down0_0 (Conv2D)  (None, 512, 512, 32  1440       ['concatenate[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " attwnetsecond_down0_0_bn (Batc  (None, 512, 512, 32  128        ['attwnetsecond_down0_0[0][0]']  \n",
      " hNormalization)                )                                                                 \n",
      "                                                                                                  \n",
      " attwnetsecond_down0_0_activati  (None, 512, 512, 32  0          ['attwnetsecond_down0_0_bn[0][0]'\n",
      " on (ReLU)                      )                                ]                                \n",
      "                                                                                                  \n",
      " attwnetsecond_down0_1 (Conv2D)  (None, 512, 512, 32  9216       ['attwnetsecond_down0_0_activatio\n",
      "                                )                                n[0][0]']                        \n",
      "                                                                                                  \n",
      " attwnetsecond_down0_1_bn (Batc  (None, 512, 512, 32  128        ['attwnetsecond_down0_1[0][0]']  \n",
      " hNormalization)                )                                                                 \n",
      "                                                                                                  \n",
      " attwnetsecond_down0_1_activati  (None, 512, 512, 32  0          ['attwnetsecond_down0_1_bn[0][0]'\n",
      " on (ReLU)                      )                                ]                                \n",
      "                                                                                                  \n",
      " attwnetsecond_down1_encode_max  (None, 256, 256, 32  0          ['attwnetsecond_down0_1_activatio\n",
      " pool (MaxPooling2D)            )                                n[0][0]']                        \n",
      "                                                                                                  \n",
      " attwnetsecond_down1_conv_0 (Co  (None, 256, 256, 64  18432      ['attwnetsecond_down1_encode_maxp\n",
      " nv2D)                          )                                ool[0][0]']                      \n",
      "                                                                                                  \n",
      " attwnetsecond_down1_conv_0_bn   (None, 256, 256, 64  256        ['attwnetsecond_down1_conv_0[0][0\n",
      " (BatchNormalization)           )                                ]']                              \n",
      "                                                                                                  \n",
      " attwnetsecond_down1_conv_0_act  (None, 256, 256, 64  0          ['attwnetsecond_down1_conv_0_bn[0\n",
      " ivation (ReLU)                 )                                ][0]']                           \n",
      "                                                                                                  \n",
      " attwnetsecond_down1_conv_1 (Co  (None, 256, 256, 64  36864      ['attwnetsecond_down1_conv_0_acti\n",
      " nv2D)                          )                                vation[0][0]']                   \n",
      "                                                                                                  \n",
      " attwnetsecond_down1_conv_1_bn   (None, 256, 256, 64  256        ['attwnetsecond_down1_conv_1[0][0\n",
      " (BatchNormalization)           )                                ]']                              \n",
      "                                                                                                  \n",
      " attwnetsecond_down1_conv_1_act  (None, 256, 256, 64  0          ['attwnetsecond_down1_conv_1_bn[0\n",
      " ivation (ReLU)                 )                                ][0]']                           \n",
      "                                                                                                  \n",
      " attwnetsecond_down2_encode_max  (None, 128, 128, 64  0          ['attwnetsecond_down1_conv_1_acti\n",
      " pool (MaxPooling2D)            )                                vation[0][0]']                   \n",
      "                                                                                                  \n",
      " attwnetsecond_down2_conv_0 (Co  (None, 128, 128, 12  73728      ['attwnetsecond_down2_encode_maxp\n",
      " nv2D)                          8)                               ool[0][0]']                      \n",
      "                                                                                                  \n",
      " attwnetsecond_down2_conv_0_bn   (None, 128, 128, 12  512        ['attwnetsecond_down2_conv_0[0][0\n",
      " (BatchNormalization)           8)                               ]']                              \n",
      "                                                                                                  \n",
      " attwnetsecond_down2_conv_0_act  (None, 128, 128, 12  0          ['attwnetsecond_down2_conv_0_bn[0\n",
      " ivation (ReLU)                 8)                               ][0]']                           \n",
      "                                                                                                  \n",
      " attwnetsecond_down2_conv_1 (Co  (None, 128, 128, 12  147456     ['attwnetsecond_down2_conv_0_acti\n",
      " nv2D)                          8)                               vation[0][0]']                   \n",
      "                                                                                                  \n",
      " attwnetsecond_down2_conv_1_bn   (None, 128, 128, 12  512        ['attwnetsecond_down2_conv_1[0][0\n",
      " (BatchNormalization)           8)                               ]']                              \n",
      "                                                                                                  \n",
      " attwnetsecond_down2_conv_1_act  (None, 128, 128, 12  0          ['attwnetsecond_down2_conv_1_bn[0\n",
      " ivation (ReLU)                 8)                               ][0]']                           \n",
      "                                                                                                  \n",
      " attwnetsecond_down3_encode_max  (None, 64, 64, 128)  0          ['attwnetsecond_down2_conv_1_acti\n",
      " pool (MaxPooling2D)                                             vation[0][0]']                   \n",
      "                                                                                                  \n",
      " attwnetsecond_down3_conv_0 (Co  (None, 64, 64, 256)  294912     ['attwnetsecond_down3_encode_maxp\n",
      " nv2D)                                                           ool[0][0]']                      \n",
      "                                                                                                  \n",
      " attwnetsecond_down3_conv_0_bn   (None, 64, 64, 256)  1024       ['attwnetsecond_down3_conv_0[0][0\n",
      " (BatchNormalization)                                            ]']                              \n",
      "                                                                                                  \n",
      " attwnetsecond_down3_conv_0_act  (None, 64, 64, 256)  0          ['attwnetsecond_down3_conv_0_bn[0\n",
      " ivation (ReLU)                                                  ][0]']                           \n",
      "                                                                                                  \n",
      " attwnetsecond_down3_conv_1 (Co  (None, 64, 64, 256)  589824     ['attwnetsecond_down3_conv_0_acti\n",
      " nv2D)                                                           vation[0][0]']                   \n",
      "                                                                                                  \n",
      " attwnetsecond_down3_conv_1_bn   (None, 64, 64, 256)  1024       ['attwnetsecond_down3_conv_1[0][0\n",
      " (BatchNormalization)                                            ]']                              \n",
      "                                                                                                  \n",
      " attwnetsecond_down3_conv_1_act  (None, 64, 64, 256)  0          ['attwnetsecond_down3_conv_1_bn[0\n",
      " ivation (ReLU)                                                  ][0]']                           \n",
      "                                                                                                  \n",
      " attwnetsecond_up0_decode_trans  (None, 128, 128, 12  295040     ['attwnetsecond_down3_conv_1_acti\n",
      " _conv (Conv2DTranspose)        8)                               vation[0][0]']                   \n",
      "                                                                                                  \n",
      " attwnetsecond_up0_decode_bn (B  (None, 128, 128, 12  512        ['attwnetsecond_up0_decode_trans_\n",
      " atchNormalization)             8)                               conv[0][0]']                     \n",
      "                                                                                                  \n",
      " attwnetsecond_up0_decode_activ  (None, 128, 128, 12  0          ['attwnetsecond_up0_decode_bn[0][\n",
      " ation (ReLU)                   8)                               0]']                             \n",
      "                                                                                                  \n",
      " attwnetsecond_up0_att_theta_x   (None, 128, 128, 64  8256       ['attwnetsecond_down2_conv_1_acti\n",
      " (Conv2D)                       )                                vation[0][0]']                   \n",
      "                                                                                                  \n",
      " attwnetsecond_up0_att_phi_g (C  (None, 128, 128, 64  8256       ['attwnetsecond_up0_decode_activa\n",
      " onv2D)                         )                                tion[0][0]']                     \n",
      "                                                                                                  \n",
      " attwnetsecond_up0_att_add (Add  (None, 128, 128, 64  0          ['attwnetsecond_up0_att_theta_x[0\n",
      " )                              )                                ][0]',                           \n",
      "                                                                  'attwnetsecond_up0_att_phi_g[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " attwnetsecond_up0_att_activati  (None, 128, 128, 64  0          ['attwnetsecond_up0_att_add[0][0]\n",
      " on (ReLU)                      )                                ']                               \n",
      "                                                                                                  \n",
      " attwnetsecond_up0_att_psi_f (C  (None, 128, 128, 1)  65         ['attwnetsecond_up0_att_activatio\n",
      " onv2D)                                                          n[0][0]']                        \n",
      "                                                                                                  \n",
      " attwnetsecond_up0_att_sigmoid   (None, 128, 128, 1)  0          ['attwnetsecond_up0_att_psi_f[0][\n",
      " (Activation)                                                    0]']                             \n",
      "                                                                                                  \n",
      " attwnetsecond_up0_att_masking   (None, 128, 128, 12  0          ['attwnetsecond_down2_conv_1_acti\n",
      " (Multiply)                     8)                               vation[0][0]',                   \n",
      "                                                                  'attwnetsecond_up0_att_sigmoid[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " attwnetsecond_up0_concat (Conc  (None, 128, 128, 25  0          ['attwnetsecond_up0_decode_activa\n",
      " atenate)                       6)                               tion[0][0]',                     \n",
      "                                                                  'attwnetsecond_up0_att_masking[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " attwnetsecond_up0_conv_after_c  (None, 128, 128, 12  294912     ['attwnetsecond_up0_concat[0][0]'\n",
      " oncat_0 (Conv2D)               8)                               ]                                \n",
      "                                                                                                  \n",
      " attwnetsecond_up0_conv_after_c  (None, 128, 128, 12  512        ['attwnetsecond_up0_conv_after_co\n",
      " oncat_0_bn (BatchNormalization  8)                              ncat_0[0][0]']                   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " attwnetsecond_up0_conv_after_c  (None, 128, 128, 12  0          ['attwnetsecond_up0_conv_after_co\n",
      " oncat_0_activation (ReLU)      8)                               ncat_0_bn[0][0]']                \n",
      "                                                                                                  \n",
      " attwnetsecond_up0_conv_after_c  (None, 128, 128, 12  147456     ['attwnetsecond_up0_conv_after_co\n",
      " oncat_1 (Conv2D)               8)                               ncat_0_activation[0][0]']        \n",
      "                                                                                                  \n",
      " attwnetsecond_up0_conv_after_c  (None, 128, 128, 12  512        ['attwnetsecond_up0_conv_after_co\n",
      " oncat_1_bn (BatchNormalization  8)                              ncat_1[0][0]']                   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " attwnetsecond_up0_conv_after_c  (None, 128, 128, 12  0          ['attwnetsecond_up0_conv_after_co\n",
      " oncat_1_activation (ReLU)      8)                               ncat_1_bn[0][0]']                \n",
      "                                                                                                  \n",
      " attwnetsecond_up1_decode_trans  (None, 256, 256, 64  73792      ['attwnetsecond_up0_conv_after_co\n",
      " _conv (Conv2DTranspose)        )                                ncat_1_activation[0][0]']        \n",
      "                                                                                                  \n",
      " attwnetsecond_up1_decode_bn (B  (None, 256, 256, 64  256        ['attwnetsecond_up1_decode_trans_\n",
      " atchNormalization)             )                                conv[0][0]']                     \n",
      "                                                                                                  \n",
      " attwnetsecond_up1_decode_activ  (None, 256, 256, 64  0          ['attwnetsecond_up1_decode_bn[0][\n",
      " ation (ReLU)                   )                                0]']                             \n",
      "                                                                                                  \n",
      " attwnetsecond_up1_att_theta_x   (None, 256, 256, 32  2080       ['attwnetsecond_down1_conv_1_acti\n",
      " (Conv2D)                       )                                vation[0][0]']                   \n",
      "                                                                                                  \n",
      " attwnetsecond_up1_att_phi_g (C  (None, 256, 256, 32  2080       ['attwnetsecond_up1_decode_activa\n",
      " onv2D)                         )                                tion[0][0]']                     \n",
      "                                                                                                  \n",
      " attwnetsecond_up1_att_add (Add  (None, 256, 256, 32  0          ['attwnetsecond_up1_att_theta_x[0\n",
      " )                              )                                ][0]',                           \n",
      "                                                                  'attwnetsecond_up1_att_phi_g[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " attwnetsecond_up1_att_activati  (None, 256, 256, 32  0          ['attwnetsecond_up1_att_add[0][0]\n",
      " on (ReLU)                      )                                ']                               \n",
      "                                                                                                  \n",
      " attwnetsecond_up1_att_psi_f (C  (None, 256, 256, 1)  33         ['attwnetsecond_up1_att_activatio\n",
      " onv2D)                                                          n[0][0]']                        \n",
      "                                                                                                  \n",
      " attwnetsecond_up1_att_sigmoid   (None, 256, 256, 1)  0          ['attwnetsecond_up1_att_psi_f[0][\n",
      " (Activation)                                                    0]']                             \n",
      "                                                                                                  \n",
      " attwnetsecond_up1_att_masking   (None, 256, 256, 64  0          ['attwnetsecond_down1_conv_1_acti\n",
      " (Multiply)                     )                                vation[0][0]',                   \n",
      "                                                                  'attwnetsecond_up1_att_sigmoid[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " attwnetsecond_up1_concat (Conc  (None, 256, 256, 12  0          ['attwnetsecond_up1_decode_activa\n",
      " atenate)                       8)                               tion[0][0]',                     \n",
      "                                                                  'attwnetsecond_up1_att_masking[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " attwnetsecond_up1_conv_after_c  (None, 256, 256, 64  73728      ['attwnetsecond_up1_concat[0][0]'\n",
      " oncat_0 (Conv2D)               )                                ]                                \n",
      "                                                                                                  \n",
      " attwnetsecond_up1_conv_after_c  (None, 256, 256, 64  256        ['attwnetsecond_up1_conv_after_co\n",
      " oncat_0_bn (BatchNormalization  )                               ncat_0[0][0]']                   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " attwnetsecond_up1_conv_after_c  (None, 256, 256, 64  0          ['attwnetsecond_up1_conv_after_co\n",
      " oncat_0_activation (ReLU)      )                                ncat_0_bn[0][0]']                \n",
      "                                                                                                  \n",
      " attwnetsecond_up1_conv_after_c  (None, 256, 256, 64  36864      ['attwnetsecond_up1_conv_after_co\n",
      " oncat_1 (Conv2D)               )                                ncat_0_activation[0][0]']        \n",
      "                                                                                                  \n",
      " attwnetsecond_up1_conv_after_c  (None, 256, 256, 64  256        ['attwnetsecond_up1_conv_after_co\n",
      " oncat_1_bn (BatchNormalization  )                               ncat_1[0][0]']                   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " attwnetsecond_up1_conv_after_c  (None, 256, 256, 64  0          ['attwnetsecond_up1_conv_after_co\n",
      " oncat_1_activation (ReLU)      )                                ncat_1_bn[0][0]']                \n",
      "                                                                                                  \n",
      " attwnetsecond_up2_decode_trans  (None, 512, 512, 32  18464      ['attwnetsecond_up1_conv_after_co\n",
      " _conv (Conv2DTranspose)        )                                ncat_1_activation[0][0]']        \n",
      "                                                                                                  \n",
      " attwnetsecond_up2_decode_bn (B  (None, 512, 512, 32  128        ['attwnetsecond_up2_decode_trans_\n",
      " atchNormalization)             )                                conv[0][0]']                     \n",
      "                                                                                                  \n",
      " attwnetsecond_up2_decode_activ  (None, 512, 512, 32  0          ['attwnetsecond_up2_decode_bn[0][\n",
      " ation (ReLU)                   )                                0]']                             \n",
      "                                                                                                  \n",
      " attwnetsecond_up2_att_theta_x   (None, 512, 512, 16  528        ['attwnetsecond_down0_1_activatio\n",
      " (Conv2D)                       )                                n[0][0]']                        \n",
      "                                                                                                  \n",
      " attwnetsecond_up2_att_phi_g (C  (None, 512, 512, 16  528        ['attwnetsecond_up2_decode_activa\n",
      " onv2D)                         )                                tion[0][0]']                     \n",
      "                                                                                                  \n",
      " attwnetsecond_up2_att_add (Add  (None, 512, 512, 16  0          ['attwnetsecond_up2_att_theta_x[0\n",
      " )                              )                                ][0]',                           \n",
      "                                                                  'attwnetsecond_up2_att_phi_g[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " attwnetsecond_up2_att_activati  (None, 512, 512, 16  0          ['attwnetsecond_up2_att_add[0][0]\n",
      " on (ReLU)                      )                                ']                               \n",
      "                                                                                                  \n",
      " attwnetsecond_up2_att_psi_f (C  (None, 512, 512, 1)  17         ['attwnetsecond_up2_att_activatio\n",
      " onv2D)                                                          n[0][0]']                        \n",
      "                                                                                                  \n",
      " attwnetsecond_up2_att_sigmoid   (None, 512, 512, 1)  0          ['attwnetsecond_up2_att_psi_f[0][\n",
      " (Activation)                                                    0]']                             \n",
      "                                                                                                  \n",
      " attwnetsecond_up2_att_masking   (None, 512, 512, 32  0          ['attwnetsecond_down0_1_activatio\n",
      " (Multiply)                     )                                n[0][0]',                        \n",
      "                                                                  'attwnetsecond_up2_att_sigmoid[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " attwnetsecond_up2_concat (Conc  (None, 512, 512, 64  0          ['attwnetsecond_up2_decode_activa\n",
      " atenate)                       )                                tion[0][0]',                     \n",
      "                                                                  'attwnetsecond_up2_att_masking[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " attwnetsecond_up2_conv_after_c  (None, 512, 512, 32  18432      ['attwnetsecond_up2_concat[0][0]'\n",
      " oncat_0 (Conv2D)               )                                ]                                \n",
      "                                                                                                  \n",
      " attwnetsecond_up2_conv_after_c  (None, 512, 512, 32  128        ['attwnetsecond_up2_conv_after_co\n",
      " oncat_0_bn (BatchNormalization  )                               ncat_0[0][0]']                   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " attwnetsecond_up2_conv_after_c  (None, 512, 512, 32  0          ['attwnetsecond_up2_conv_after_co\n",
      " oncat_0_activation (ReLU)      )                                ncat_0_bn[0][0]']                \n",
      "                                                                                                  \n",
      " attwnetsecond_up2_conv_after_c  (None, 512, 512, 32  9216       ['attwnetsecond_up2_conv_after_co\n",
      " oncat_1 (Conv2D)               )                                ncat_0_activation[0][0]']        \n",
      "                                                                                                  \n",
      " attwnetsecond_up2_conv_after_c  (None, 512, 512, 32  128        ['attwnetsecond_up2_conv_after_co\n",
      " oncat_1_bn (BatchNormalization  )                               ncat_1[0][0]']                   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " attwnetsecond_up2_conv_after_c  (None, 512, 512, 32  0          ['attwnetsecond_up2_conv_after_co\n",
      " oncat_1_activation (ReLU)      )                                ncat_1_bn[0][0]']                \n",
      "                                                                                                  \n",
      " attwnetsecond_output (Conv2D)  (None, 512, 512, 4)  132         ['attwnetsecond_up2_conv_after_co\n",
      "                                                                 ncat_1_activation[0][0]']        \n",
      "                                                                                                  \n",
      " attwnetsecond_output_activatio  (None, 512, 512, 4)  0          ['attwnetsecond_output[0][0]']   \n",
      " n (Softmax)                                                                                      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,335,406\n",
      "Trainable params: 4,328,878\n",
      "Non-trainable params: 6,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_alias = f\"1_Attention_WNet_normal\"\n",
    "model = att_wnet_2d((Image_Size, Image_Size, 1), filter_num=[32, 64, 128, 256], n_labels1 =n_labels, n_labels2=n_labels,\n",
    "                                stack_num_down=2, stack_num_up=2, activation='ReLU', \n",
    "                                atten_activation='ReLU', attention='add', output_activation='Softmax', \n",
    "                                batch_norm=True, pool='max', unpool=False, backbone=None, weights=None, \n",
    "                                name='attwnet')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-22T09:20:14.193648Z",
     "iopub.status.idle": "2024-09-22T09:20:14.194480Z",
     "shell.execute_reply": "2024-09-22T09:20:14.194243Z",
     "shell.execute_reply.started": "2024-09-22T09:20:14.194221Z"
    }
   },
   "outputs": [],
   "source": [
    "model_alias = f\"1_swin_Wnet_normal\"\n",
    "#model = swin_wnet_2d( (Image_Size,Image_Size,1) , filter_num_begin=16, n_labels1=4, n_labels2=4,\n",
    "                                    depth=3, stack_num_down=2, stack_num_up=2, patch_size=(2, 2), \n",
    "                                    num_heads=[4, 8, 8, 8], window_size=[4, 2, 2, 2], num_mlp=128, \n",
    "                                output_activation='Softmax', shift_window=True, name=model_alias)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-09-22T09:21:51.320393Z",
     "iopub.status.busy": "2024-09-22T09:21:51.319711Z"
    },
    "id": "r9zFppwsLJk2",
    "outputId": "6c4c07a6-cbef-4868-f1d6-b8fe9c9eed0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_Attention_WNet_normal_DataAug_DS_DRIVE_AV_IS512_EPS200_OPSAdam_LSfocal_loss_BS6_LR001\n",
      "Epoch 1/200\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.7599\n",
      "Epoch 1: val_loss improved from inf to 1.20377, saving model to 1_Attention_WNet_normal_DataAug_DS_DRIVE_AV_IS512_EPS200_OPSAdam_LSfocal_loss_BS6_LR001\n",
      "27/27 [==============================] - 92s 2s/step - loss: 0.0875 - accuracy: 0.7599 - val_loss: 1.2038 - val_accuracy: 0.9080\n",
      "Epoch 2/200\n"
     ]
    }
   ],
   "source": [
    "save_model_name = model_alias + \"_DataAug_DS_\" + Dataset +\"_IS\" + str(Image_Size) +\"_EPS\" + str(no_epoch) +\"_OPS\" + optimizers +\"_LS\" + losses +\"_BS\" + str(batch_sizes) +\"_LR\" + str(learning_rate).replace(\".\", '')\n",
    "callbacks_list = generate_callbacks(model, \"min\", save_model_name,\"val_loss\" , 25, np.array([TestX[0]]), np.array([TestY[0]]) )\n",
    "loss = generate_loss(losses)\n",
    "optimizer = generate_optimizer(optimizers, learning_rate)\n",
    "print(save_model_name)\n",
    "model.compile(loss=loss, optimizer= optimizer, metrics=['accuracy'])\n",
    "# model = keras.models.load_model('/home/subin/avSegmentation/1_Attention_WNet_normal_Split08_DRIVE_AV_512_500_Adam_focal_loss_4_00001', custom_objects={'focal_weighted':                   \n",
    "# focal_weighted})\n",
    "# print(model.optimizer.lr)\n",
    "# model.optimizer.lr  = 1e-6\n",
    "# print(model.optimizer.lr)\n",
    "\n",
    "model.fit(TrainX, TrainY, validation_data=(TestX, TestY), batch_size=batch_sizes, epochs=no_epoch, verbose=1, shuffle=True, callbacks= callbacks_list)\n",
    "# model.fit_generator(generator=train_generator, epochs=10, steps_per_epoch=len(train_generator), validation_data=test_generator)\n",
    "# model.fit(x=train_generator, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-22T09:20:14.198078Z",
     "iopub.status.idle": "2024-09-22T09:20:14.198455Z",
     "shell.execute_reply": "2024-09-22T09:20:14.198270Z",
     "shell.execute_reply.started": "2024-09-22T09:20:14.198254Z"
    }
   },
   "outputs": [],
   "source": [
    "# swin_wnet = keras.models.load_model('/home/subin/avSegmentation/1_Attention_WNet_normal_Split08_DRIVE_AV_500_Adam_focal_loss_4_00001', custom_objects={'focal_weighted':                   \n",
    "# focal_weighted})\n",
    "\n",
    "# # swin_wnet_old = keras.models.load_model('/home/subin/avSegmentation/Data_Aug_HRF1024_Green_Adam_1e2_focal_bs4_ep500', custom_objects={'focal_weighted':                   \n",
    "# # focal_weighted})# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-22T09:20:14.200427Z",
     "iopub.status.idle": "2024-09-22T09:20:14.200811Z",
     "shell.execute_reply": "2024-09-22T09:20:14.200644Z",
     "shell.execute_reply.started": "2024-09-22T09:20:14.200621Z"
    }
   },
   "outputs": [],
   "source": [
    "# swin_wnet_predicted_Test = swin_wnet.predict(TestX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-22T09:20:14.201860Z",
     "iopub.status.idle": "2024-09-22T09:20:14.202998Z",
     "shell.execute_reply": "2024-09-22T09:20:14.202800Z",
     "shell.execute_reply.started": "2024-09-22T09:20:14.202772Z"
    }
   },
   "outputs": [],
   "source": [
    "# swin_wnet_predicted_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-22T09:20:14.204190Z",
     "iopub.status.idle": "2024-09-22T09:20:14.204551Z",
     "shell.execute_reply": "2024-09-22T09:20:14.204374Z",
     "shell.execute_reply.started": "2024-09-22T09:20:14.204358Z"
    }
   },
   "outputs": [],
   "source": [
    "# swin_wnet_predicted_Train = swin_wnet.predict(TrainX)\n",
    "# swin_wnet_predicted_Test = swin_wnet.predict(TestX)\n",
    "\n",
    "import numpy as np\n",
    "from skimage.morphology import skeletonize, erosion\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def evaluation_code(prediction, groundtruth):\n",
    "    \n",
    "    encoded_pred = np.zeros(prediction.shape[:2], dtype=int)\n",
    "    encoded_gt = np.zeros(groundtruth.shape[:2], dtype=int)\n",
    "    \n",
    "    # convert white pixels to green pixels (which are ignored)\n",
    "    white_ind = np.where(np.logical_and(groundtruth[:,:,0] == 255, groundtruth[:,:,1] == 255, groundtruth[:,:,2] == 255))\n",
    "    if white_ind[0].size != 0:\n",
    "        groundtruth[white_ind] = [0,255,0]\n",
    "        \n",
    "    # translate the images to arrays suited for sklearn metrics\n",
    "    arteriole = np.where(np.logical_and(groundtruth[:,:,0] == 255, groundtruth[:,:,1] == 0)); encoded_gt[arteriole] = 1\n",
    "    venule = np.where(np.logical_and(groundtruth[:,:,2] == 255, groundtruth[:,:,1] == 0)); encoded_gt[venule] = 2\n",
    "    arteriole = np.where(prediction[:,:,0] == 255); encoded_pred[arteriole] = 1\n",
    "    venule = np.where(prediction[:,:,2] == 255); encoded_pred[venule] = 2\n",
    "   \n",
    "    # retrieve the indices for the centerline pixels present in the prediction\n",
    "    center = np.where(np.logical_and(\n",
    "        np.logical_or((skeletonize(groundtruth[:,:,0] > 0)),(skeletonize(groundtruth[:,:,2] > 0))),\n",
    "        encoded_pred[:,:] > 0))\n",
    "    \n",
    "    encoded_pred_center = encoded_pred[center]\n",
    "    encoded_gt_center = encoded_gt[center]\n",
    "    \n",
    "    # retrieve the indices for the centerline pixels present in the groundtruth\n",
    "    center_comp = np.where(\n",
    "        np.logical_or(skeletonize(groundtruth[:,:,0] > 0),skeletonize(groundtruth[:,:,2] > 0)))\n",
    "    \n",
    "    encoded_pred_center_comp = encoded_pred[center_comp]\n",
    "    encoded_gt_center_comp = encoded_gt[center_comp]\n",
    "    \n",
    "    # retrieve the indices for discovered centerline pixels - limited to vessels wider than two pixels (for DRIVE)\n",
    "    center_eroded = np.where(np.logical_and(\n",
    "        np.logical_or(skeletonize(erosion(groundtruth[:,:,0] > 0)),skeletonize(erosion(groundtruth[:,:,2] > 0))),\n",
    "        encoded_pred[:,:] > 0))\n",
    "                             \n",
    "    encoded_pred_center_eroded = encoded_pred[center_eroded]\n",
    "    encoded_gt_center_eroded = encoded_gt[center_eroded]\n",
    "    \n",
    "    # metrics over full image\n",
    "    cur1_acc = accuracy_score(encoded_gt.flatten(),encoded_pred.flatten())\n",
    "    cur1_F1 = f1_score(encoded_gt.flatten(),encoded_pred.flatten(),average='weighted')\n",
    "    print('Full image')\n",
    "    print('Accuracy: {}\\nF1: {}\\n'.format(cur1_acc, cur1_F1))\n",
    "    metrics1 = [cur1_acc, cur1_F1]\n",
    "    \n",
    "    # metrics over discovered centerline pixels\n",
    "    cur2_acc = accuracy_score(encoded_gt_center.flatten(),encoded_pred_center.flatten())\n",
    "    cur2_F1 = f1_score(encoded_gt_center.flatten(),encoded_pred_center.flatten(),average='weighted')\n",
    "    print('Discovered centerline pixels')\n",
    "    print('Accuracy: {}\\nF1: {}\\n'.format(cur2_acc, cur2_F1))\n",
    "    metrics2 = [cur2_acc, cur2_F1]\n",
    "    \n",
    "    # metrics over discovered centerline pixels - limited to vessels wider than two pixels\n",
    "    cur3_acc = accuracy_score(encoded_gt_center_eroded.flatten(),encoded_pred_center_eroded.flatten())\n",
    "    cur3_F1 = f1_score(encoded_gt_center_eroded.flatten(),encoded_pred_center_eroded.flatten(),average='weighted')\n",
    "    print('Discovered centerline pixels of vessels wider than two pixels')\n",
    "    print('Accuracy: {}\\nF1: {}\\n'.format(cur3_acc, cur3_F1))\n",
    "    metrics3 = [cur3_acc, cur3_F1]\n",
    "    \n",
    "    # metrics over all centerline pixels in ground truth\n",
    "    cur4_acc = accuracy_score(encoded_gt_center_comp.flatten(),encoded_pred_center_comp.flatten())\n",
    "    cur4_F1 = f1_score(encoded_gt_center_comp.flatten(),encoded_pred_center_comp.flatten(),average='weighted')\n",
    "    print('Centerline pixels')\n",
    "    print('Accuracy: {}\\nF1: {}\\n'.format(cur4_acc, cur4_F1))\n",
    "    metrics4 = [cur4_acc, cur4_F1]\n",
    "    \n",
    "    # finally, compute vessel detection rate\n",
    "    vessel_ind = np.where(encoded_gt>0)\n",
    "    vessel_gt = encoded_gt[vessel_ind]\n",
    "    vessel_pred = encoded_pred[vessel_ind]\n",
    "    \n",
    "    detection_rate = [accuracy_score(vessel_gt.flatten(),vessel_pred.flatten()),0]\n",
    "    print('Amount of vessels detected: ' + str(detection_rate))\n",
    "    \n",
    "    return [metrics1,metrics2,metrics3,metrics4,detection_rate]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-22T09:20:14.206284Z",
     "iopub.status.idle": "2024-09-22T09:20:14.206632Z",
     "shell.execute_reply": "2024-09-22T09:20:14.206475Z",
     "shell.execute_reply.started": "2024-09-22T09:20:14.206458Z"
    }
   },
   "outputs": [],
   "source": [
    "# def create_mask(pred_mask):\n",
    "#     new_label = np.zeros((Image_Size,Image_Size,3))\n",
    "#     for i in range(Image_Size):\n",
    "#         for j in range(Image_Size):\n",
    "#             index = np.argmax(pred_mask[i][j])\n",
    "#             if(index>0):\n",
    "#                 new_label[i][j][index-1]=255\n",
    "#     return new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-22T09:20:14.208346Z",
     "iopub.status.idle": "2024-09-22T09:20:14.208728Z",
     "shell.execute_reply": "2024-09-22T09:20:14.208570Z",
     "shell.execute_reply.started": "2024-09-22T09:20:14.208550Z"
    }
   },
   "outputs": [],
   "source": [
    "# for i in range(len(swin_wnet_predicted_Test)):\n",
    "#     plt.imshow(create_mask(swin_wnet_predicted_Test[i,:,:,:]))\n",
    "#     plt.show()\n",
    "#     plt.imshow(TestY[i,:,:,1:4])\n",
    "#     plt.show()\n",
    "# #     print(\"Image No:\",i,\": \",evaluation_code(swin_wnet_predicted_Test[i,:,:,1:4], swin_wnet_predicted_Test_Label[i,:,:,1:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
